---
title: "Stat 324 Homework 8"
author: "Zinnia Nie"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Submit your homework to Canvas by the due date and time.* **Because this homework is due right before the exam, we will not be able to give extensions on it.**

*If an exercise asks you to use R, include a copy of the code and output. Please edit your code and output to be only the relevant portions.

*If a problem does not specify how to compute the answer, you many use any appropriate method. I may ask you to use R or use manually calculations on your exams, so practice accordingly.

*You must include an explanation and/or intermediate calculations for an exercise to be complete.

*Be sure to submit the HWK 8 Auto grade Quiz which will give you ~20 of your 40 accuracy points.

*50 points total: 40 points accuracy, and 10 points completion


## Hypothesis Testing and CI for $\mu_1-\mu_2$

**Exercise 1** Data on household vehicle miles of travel (VMT) are compiled annually by the Federal Highway Administration. A researcher is interested in whether there is a difference in last year's mean VMT for Midwestern and southern households ($\mu_M$ and $\mu_S$). Independent random samples of 15 Midwestern households and 14 southern households provided the following data on last year's VMT, in thousands of miles:

$$Midwest: 16.2, 12.9, 17.3, 14.6, 18.6, 10.8, 11.2, 16.6, 16.6, 24.4, 20.3, 20.9, 9.6, 15.1, 18.3$$
$$South: 22.2, 19.2, 10.3, 24.6, 20.2, 15.8, 18.0, 13.6, 20.1, 16.0, 17.5, 18.2, 22.8, 11.8$$

> a. Graph the data as you see fit. Why did you choose the graph(s) you did and what do they tell you?  Also calculate summary statistics relevant to the research question.

```{r}
midwest = c(16.2, 12.9, 17.3, 14.6, 18.6, 10.8, 11.2, 16.6, 16.6, 24.4, 20.3, 20.9, 9.6, 15.1, 18.3)
south = c(22.2, 19.2, 10.3, 24.6, 20.2, 15.8, 18.0, 13.6, 20.1, 16.0, 17.5, 18.2, 22.8, 11.8)

hist(midwest, breaks = seq(4, 26, 2))
hist(south)
hist(south)

(m_m = mean(midwest))
(m_s = mean(south))
(s_m = sd(midwest))
(s_s = sd(south))
s_m/s_s
(n_m = length(midwest))
(n_s = length(south))

m_m-m_s
```
I chose a histogram because I wanted to see the shape of the distribution of the sample values. 
This is not representative of the population distribution, but it may give a little insight into how the population is distributed. In this case, the midwest values look to follow a normal distribution, with a tall peak in the center and relatively even distribution on either side. The south values are distributed more symmetrically, but still forms a slight bell curve.

Summary statistics needed for the research question are sample mean, sample distribution, and sample size. Also, since we are looking at the difference in means, I also calculated the difference in the sample means.

> b. Perform a $10\%$ significance level two sample t test for means **assuming equal variance** to address the researcher's question. Justify why the assumptions of the test are reasonably met or describe what assumptions we are assuming are met. As part of this test, specify your *hypotheses*, calculate your *test statistic*, *p value* and make a *conclusion in the context* of the question (Bold or highlight these values in your solutions). Show all steps of the computation and then check your computations using t.test.

>> *Assumptions:*

>>> (1) IID Samples from Independent Populations?

  As stated by the researcher, the two samples are independent random samples. The two populations are also independent, because the values are not influenced by the other population, as they are from two different parts of the country.

>>> (2) Normality?

  There is some evidence that the populations are normally distributed, since the distribution of the sample values for the midwest and the south look normal, but we are unsure how that extends to the population, and the sample size is not big enough to make the central limit theorem apply. Thus we would need to further clarify/test whether the two distributions are normal or not so that we can use the t test.

>>> (3) Equal Variance?

  The two sample variances are very close. To be more specific, the ratio is within a factor of 2, as $0.5<0.9836225<2$ and the sample sizes are only 1 apart, so we can assume that the variances are equal.
  
>> *Hypotheses:* 

$H_0 : \mu_M - \mu_S = 0$

$H_A : \mu_M - \mu_S \ne 0$

>> *Computations:*

>>> Test Statistic

$s_p^2 = \frac{(n_X-1)s_S^2 + (n_Y-1)s_Y^2}{n_X+n_Y-2} = \frac{(15-1)4.055062^2 + (14-1)4.122579^2}{15+14-2} = \frac{(14)16.44353 + (13)16.99566}{27} = \frac{451.153}{27} = 16.70937$

$s_p = \sqrt{16.70937} = 4.08771$

$t = \frac{(\bar{X}-\bar{Y})-0}{s_p\sqrt{\frac{1}{n_X}+\frac{1}{n_Y}}} = \frac{(16.22667-17.87857)-0}{4.08771\sqrt{\frac{1}{15}+\frac{1}{14}}} = \frac{-1.6519}{4.08771\sqrt{0.1380952}} = \frac{-1.6519}{4.08771\sqrt{0.1380952}} = \frac{-1.6519}{1.519041} = -1.087462$

**t = -1.087462**

>>> P-Value 

```{r}
2*pt(-1.087462, 27)
```


**p-value = 0.2864455**

>>> Computation Check

```{r}
t.test(midwest, south, alternative = "two.sided", var.equal=TRUE, conf.level = 0.90)
```

>> *Conclusion:*

Because the p value of $0.2864$ is greater than the significance level of 0.10, we do not have enough evidence that the difference in population means is not equal to 0. Therefore, we **fail to reject the null hypothesis**.

> c. OPTIONAL PRACTICE A confidence interval for the true difference in mean VMT $\mu_M-\mu_S$ assuming equal population variances is reported as: (-3.647, 0.344). Identify the point estimate, margin of error, critical value, degrees of freedom, standard error or the estimator, and confidence level used to construct it.  Discuss the relationship between your findings in b and c.

>> *Point estimate for $\mu_M-\mu_S$* 

$\mu_M-\mu_S = -3.647 + (0.344-(-3.647))/2 = -3.647 + 3.991/2 = -1.6515$

>> *Margin of Error*

$(0.344-(-3.647))/2 = 3.991/2 = 1.9955$

>> *Standard Error of Estimator:* 

Same as what was used above, so should be $1.519041$

10% significance level:

```{r}
qt(0.05, 27, lower.tail = FALSE)
```

>> *Degrees of Freedom*: 

Same as above, the $df = 27$

>> *Critical Value*:

Margin of error = $t_{\frac{\alpha}{2}}*SE$

$t_{\frac{\alpha}{2}}$ = Margin of error / SE

$t_{\frac{\alpha}{2}} = 1.9955 / 1.519041 = 1.313658$

>> *Confidence Interval Level* 

```{r}
pt(1.313658, 27)

qt(0.10, 27)
```

The critical value gives around 0.90 on the t distribution, so $t_{\frac{\alpha}{2}} = 0.10$ and then $\alpha = 0.20$. Therefore, the confidence level for this interval is 80%.

>> *Comparison between b and c findings:*

The calculated point estimate and standard error is also used in the calculation of test statistic. The degrees of freedom are the same as the calculation of p value, but the R function used is different.

> d. OPTIONAL PRACTICE Compute the test statistic and p value for $H_o: \mu_M-\mu_S =0$ vs $H_A: \mu_M -\mu_S \ne 0$ and 95% confidence interval for $\mu_M-\mu_S$ **not** assuming equal population variances. You can use t.test(), but make sure you could also compute these values by hand (other than df). How does the difference in population variances assumptions change how we do the calculations in the two independent sample t test we perform?

```{r}
t.test(midwest, south, alternative = "two.sided", var.equal=FALSE, conf.level = 0.90)
```

$df = 26.792$

$s_p^2 = \frac{(n_X-1)s_S^2 + (n_Y-1)s_Y^2}{n_X+n_Y-2} = \frac{(15-1)4.055062^2 + (14-1)4.122579^2}{15+14-2} = \frac{(14)16.44353 + (13)16.99566}{27} = \frac{451.153}{27} = 16.70937$

**Test Statistic**

$t = \frac{(\bar{X}-\bar{Y})-0}{\sqrt{\frac{s_X^2}{n_X}+\frac{s_Y^2}{n_Y}}} = \frac{(16.22667-17.87857)-0}{\sqrt{\frac{4.055062^2}{15}+\frac{4.122579^2}{14}}} = \frac{-1.6519}{\sqrt{2.310211}} = \frac{-1.6519}{1.519938} = -1.086821$

**t = -1.086821**

**p value (use df=26.792)**  

```{r}
2*pt(-1.086821, 26.792)
```

**p value = 0.2867975**

**Confidence interval:** $-1.6519 - 0.5098799*1.519938, -1.6519 + 0.5098799*1.519938 = (-2.426886, -0.8769142)$

```{r}
pt(0.025, 26.792)
```

The only calculation that changes will be that we do not calculated a pooled standard deviation $s_p$, instead we use a different formula in order to estimate $s_{\bar{X}-\bar{Y}}$. The changes our calculation very slightly in this case because the two variances for the samples were already very close and could be assmed equal.

**

\vspace{.5 cm}

**Exercise 2.** In a study of the relationship of the shape of a tablet to its dissolution time, disk-shaped ibuprofen tablets and oval-shaped ibuprofen tablets were dissolved in water.A normality assumption for dissolution time has been used in the past. Researchers wonder if the mean dissolution time is different for the two different shapes. The dissolve time, in seconds, for this experiment were as follows:

$$Disk: 268.0, 249.3, 255.2, 252.7, 254.0, 261.6, 248.3, 266.7, 254.8, 255.1, 262.3, 255.3, 251.0, 253.4$$
$$Oval: 268.8, 260.0, 273.5, 251.7, 278.5, 289.4, 261.6, 253.0, 262.4$$

Numeric summaries of the samples:

| Shape | Sample Mean | Sample Median | Sample Variance | Sample Size (n) |
| :-: | :-: | :-: | :-: | :-: |
| Disk | 256.2643 | 254.95| 37.13632 | 14 |
| Oval | 266.5444 | 262.4 | 150.8803 | 9 |


> a. Graph the data as you see fit. How do those graphs align with the normality assumption that has been used in the past?

```{r}
disk = c(268.0, 249.3, 255.2, 252.7, 254.0, 261.6, 248.3, 266.7, 254.8, 255.1, 262.3, 255.3, 251.0, 253.4)
oval = c(268.8, 260.0, 273.5, 251.7, 278.5, 289.4, 261.6, 253.0, 262.4)

hist(disk)
hist(oval)
```

Neither of the distributions look to be normally distributed. Instead they both look right skewed as they have a concentration of values on the left and a long tail to the right. This does not match with the normality assumption that has been used in the past, which brings this assumption into question. However, since these are samples, it is uncertain how the distribution of the actual population would look like, as it could still be normal and only the samples are right skewed.

> b. Perform a two independent sample t test for means at the $5\%$ level **allowing the population variances to differ** and describe what assumptions this test is assuming are met. As part of this test, specify your hypotheses, calculate a test statistic and p value and make a conclusion in the context of the question. Do the calculations by hand and check your computations with t.test() in R.

*Assumptions:*

Independence: The population of the two types of tablets are likely to be greater than 14 * 10=140 and 9 * 10=90 respectively. Therefore, each sample has independent values. To check the independence between the two populations, we would have to ask about the experiment performed. If the tablets were each tested separately and all other variables were controlled, the two populations are also independent of each other. Finally, we would also have to ask about the sampling method to see if the two are simple random samples. 

Normality: We will continue to use the normality assumption from the past. 

We are letting the population variances to differ, so we must estimate $\sigma_{\bar{X}-\bar{Y}}^2 \approx \frac{s_X^2}{n_X}+\frac{s_Y^2}{n_Y}$

*Hypotheses:* 

$H_0 : \mu_D - \mu_O = 0$

$H_A : \mu_D - \mu_O \ne 0$

*Computations:*

> **Test Statistic**

$t = \frac{(\bar{X}-\bar{Y})-0}{\sqrt{\frac{s_X^2}{n_X}+\frac{s_Y^2}{n_Y}}} = \frac{(256.2643-266.5444)-0}{\sqrt{\frac{37.13632}{14}+\frac{150.8803}{9}}} = \frac{-10.2801}{\sqrt{2.652594+16.76448}} = \frac{-10.2801}{\sqrt{19.41707}} = \frac{-10.2801}{4.40648} = -2.332951$

**t = -2.332951**

> **p value (use df=10.56909)**  

```{r}
2*pt(-2.332951, 10.56909)
```

**p value = 0.0405469**

> Check Calculations

```{r}
t.test(disk, oval)
```

*Conclusion:*

The p value, $0.0405469$, is lower than the significance level of 0.05, therefore we have enough evidence that shows there is a difference in the population means of disk and oval tablet shapes. We will thus **reject the null hypothesis**.

\vspace{.5 cm} 

> c. A two-sided t confidence interval for $\mu_D-\mu_O$ allowing population variances to vary is reported as (-16.3038, -4.2566). Identify the level of the [two-sided] confidence interval. You can use the summary measures given earlier. Interpret this confidence interval in the context of the question.

Difference in sample means is -10.2801. Therefore, the margin of error is 6.0235. Estimated population standard deviation is $\sigma_{\bar{X}-\bar{Y}} \approx \sqrt{\frac{s_X^2}{n_X}+\frac{s_Y^2}{n_Y}}$ which is 4.40648. Now we need to look at the t critical value, which will be margin of error / estimated $\sigma_{\bar{X}-\bar{Y}}$ = $6.0235/4.40648 = 1.366964$ Using a t value of 1.366986 in the qt() function in R, we get a percentile of the t distribution of 0.9, and $\alpha = 0.2$. This means that confidence level is 80%, because we divide alpha by 2 to get the t critical value. 

```{r}
pt(1.366986, 10.56909)
```

In the context of the situation, this means we are 80% confident that the difference in population means of shapes of tablets lies in the interval. 

> d. One of your lab mates suggests not making the normality assumptions necessary for the t test and instead using a bootstrap tool. So, use the bootstrap code provided in lecture to 

>> di. perform a bootstrap hypothesis test of the hypotheses $H_0: \mu_D-\mu_O = 0$ vs  $H_A: \mu_D-\mu_O \ne 0$ and compare the assumptions, test statistic, p value, and conclusions of this test to the Welch's you performed in part (b). 

```{r}
boottwo = function(dat1, dat2, nboot) {
  bootstat = numeric(nboot)		#Make Empty Vector for t* to fill
  obsdiff = mean(dat1) - mean(dat2)
  n1 = length(dat1)
  n2 = length(dat2)
  for(i in 1:nboot) {
    samp1 = sample(dat1, size = n1, replace = T)	#Sample From Sample Data
    samp2 = sample(dat2, size = n2, replace = T)
    bootmean1 = mean(samp1)
    bootmean2 = mean(samp2)
    bootvar1 = var(samp1)
    bootvar2 = var(samp2)
    bootstat[i] = ((bootmean1 - bootmean2) - obsdiff)/sqrt((bootvar1/n1) + (bootvar2/n2))		#Compute and Save “bootstrap t” value
}
  return(bootstat)
}

#bootstrap
times.boot<-boottwo(disk, oval, nboot=5000)

p = 2*min(sum(times.boot<=-2.332951)/5000, sum(times.boot>=-2.332951)/5000)

cat('Bootstrap p-value =', p)
```

For the assumptions, we no longer assume normality when performing a bootstrap test, since we will be creating out own distribution by resampling. The test statistic is still the same as the Welch's two sample t test. However, the p value is different now. The calculated p value for bootstrapping is about 0.02, although it does change because each bootstrap distribution is different, compared to the Welch's test p value of 0.0405469. The conclusion for both tests are the same though, as both p values are less than the significance level of 0.05, and we thus reject the null hypothesis.

>> dii. Construct a 99 \% bootstrap t confidence interval for the true difference in mean dissolution time: $\mu_D-\mu_O$. Interpret the confidence interval in the context of the question

```{r}
(lower<-quantile(times.boot, .005, names=FALSE))
(upper<-quantile(times.boot, .995, name=FALSE))

(diff = mean(disk)-mean(oval))

cat('Upper bound =', diff-(lower*4.40648))
cat('\n')
cat('Lower bound =', diff-(upper*4.40648))
```
We are 99% confident that the true difference in population means for dissolution times for disk and oval shaped tablets is between about 2 and -27.  

> e. Suppose after the data is collected it comes to your attention that the researcher performed all of the Disk dissolution data collection in the afternoon when it was warmer and the overhead fan was on in the lab. The Oval dissolution data was collected in the morning when the lab was cool and the fan was not running. What, if any, concerns do you have about the data collection? What is your response.

Because all variables are not controlled, the difference in the testing environment might affect the variable we are testing. It is possible that temperature and presence of a fan would affect dissolution times of the tablets. Therefore, we cannot confidently compare both populations because the inference we are making might not be able to be applied to the different environmental conditions. I would ask the experiment to be performed again with all variables controlled so that the two samples could be better compared. 

\vspace{.5cm}


**Exercise 3.** The urinary fluoride concentration (ppm) was measured both for a sample of livestock that had been grazing in an area previously exposed to flouride pollution and for a similar sample that had grazed in an unpolluted region. Due to the smaller sample sizes and some evidence of nonnormality seen in the samples, the researchers plan to perform a Wilcoxon Rank-Sum/Mann-Whitney Test.

|Environment| Sample Values|
|:-:| :-: | 
|Polluted |  21.3, 18.7, 23.0, 17.1, 16.8, 20.9, 19.7 |
|Unpolluted | 14.2, 18.3, 17.2, 18.4, 20.0 |

> a. What assumptions must be made for a Wilcoxon Rank Sum Test to be reasonable?

We must assume that both samples are simple random samples of iid draws and that the two populations are also independent from each other. 

We can ask the researchers about their sampling method to see if the samples are simple random and independent. We also have to see if the two different groups of livestock have any interactions or were grazed in different places, as that may cause the two populations to no longer be independent.

> b. Perform a Wilcoxon Rank Sum Test in R of the research hypotheses: $H_A:$ the distribution of urinary flouride concentrations in lifestock from polluted grazing pastures is the same as the distribution of urinary flouride concentrations in lifestock from unpolluted grazing pastures except for a shift to the right in those from polluted pastures. Draw a conclusion at the $\alpha=0.05$ level in context. You may need to use `?wilcox.test` to see how to set a one-sided alternative.

```{r}
polluted = c(21.3, 18.7, 23.0, 17.1, 16.8, 20.9, 19.7)
unpolluted = c(14.2, 18.3, 17.2, 18.4, 20.0)

wilcox.test(x=polluted, y = unpolluted ,alternative = "greater",	mu = 0, paired = FALSE, exact = NULL, correct = TRUE, conf.int = FALSE, conf.level = 0.95)
```
The p-value for a Wilcoxon Test is 0.1338. This is greater than the significance level of $\alpha=0.05$, so we fail to reject the null hypothesis that the distributions of urinary flouride concentrations in livestock in polluted and unpolluted pastures are identical.
